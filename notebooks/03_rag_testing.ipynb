{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# RAG System Testing\\n\",\n",
    "    \"## End-to-end testing of the RAG pipeline with BiomedLM\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook tests:\\n\",\n",
    "    \"- Query embedding and retrieval\\n\",\n",
    "    \"- Context construction\\n\",\n",
    "    \"- BiomedLM response generation\\n\",\n",
    "    \"- System performance metrics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"from typing import List, Dict\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.embedding.model_loader import ModelLoader\\n\",\n",
    "    \"from src.embedding.embedder import EmbeddingGenerator\\n\",\n",
    "    \"from src.indexing.faiss_indexer import FAISSIndexer\\n\",\n",
    "    \"from src.indexing.retriever import Retriever\\n\",\n",
    "    \"from src.llm.biomedlm import BiomedLMGenerator\\n\",\n",
    "    \"from src.llm.prompt_builder import PromptBuilder\\n\",\n",
    "    \"from src.config import settings\\n\",\n",
    "    \"\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (12, 6)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"✓ Imports successful\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Initialize RAG Components\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"Initializing RAG components...\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load embedding model\\n\",\n",
    "    \"print(\\\"1. Loading embedding model...\\\")\\n\",\n",
    "    \"model_loader = ModelLoader(settings.EMBEDDING_MODEL, settings.DEVICE)\\n\",\n",
    "    \"tokenizer, model = model_loader.load()\\n\",\n",
    "    \"embedder = EmbeddingGenerator(tokenizer, model, settings.DEVICE, batch_size=32)\\n\",\n",
    "    \"print(\\\"   ✓ Embedding model loaded\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load FAISS index\\n\",\n",
    "    \"print(\\\"\\\\n2. Loading FAISS index...\\\")\\n\",\n",
    "    \"faiss_indexer = FAISSIndexer()\\n\",\n",
    "    \"if Path(settings.INDEX_PATH).exists():\\n\",\n",
    "    \"    faiss_indexer.load(settings.INDEX_PATH)\\n\",\n",
    "    \"    print(f\\\"   ✓ FAISS index loaded ({faiss_indexer.index.ntotal} vectors)\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"   ❌ Index not found! Run: python scripts/build_index.py\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load chunks\\n\",\n",
    "    \"print(\\\"\\\\n3. Loading chunks metadata...\\\")\\n\",\n",
    "    \"if Path(settings.CHUNKS_PATH).exists():\\n\",\n",
    "    \"    with open(settings.CHUNKS_PATH, 'r') as f:\\n\",\n",
    "    \"        chunks = json.load(f)\\n\",\n",
    "    \"    print(f\\\"   ✓ Loaded {len(chunks)} chunks\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"   ❌ Chunks not found!\\\")\\n\",\n",
    "    \"    chunks = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize retriever\\n\",\n",
    "    \"print(\\\"\\\\n4. Initializing retriever...\\\")\\n\",\n",
    "    \"retriever = Retriever(faiss_indexer, chunks, top_k=settings.TOP_K_RETRIEVAL)\\n\",\n",
    "    \"print(\\\"   ✓ Retriever ready\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load BiomedLM\\n\",\n",
    "    \"print(\\\"\\\\n5. Loading BiomedLM (this may take a while)...\\\")\\n\",\n",
    "    \"llm_generator = BiomedLMGenerator(\\n\",\n",
    "    \"    settings.LLM_MODEL,\\n\",\n",
    "    \"    settings.DEVICE,\\n\",\n",
    "    \"    settings.MAX_NEW_TOKENS,\\n\",\n",
    "    \"    settings.TEMPERATURE,\\n\",\n",
    "    \"    settings.TOP_P\\n\",\n",
    "    \")\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    llm_generator.load()\\n\",\n",
    "    \"    print(\\\"   ✓ BiomedLM loaded\\\")\\n\",\n",
    "    \"    llm_available = True\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"   ⚠ BiomedLM not available: {e}\\\")\\n\",\n",
    "    \"    print(\\\"   Will test retrieval only\\\")\\n\",\n",
    "    \"    llm_available = False\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize prompt builder\\n\",\n",
    "    \"print(\\\"\\\\n6. Initializing prompt builder...\\\")\\n\",\n",
    "    \"prompt_builder = PromptBuilder()\\n\",\n",
    "    \"print(\\\"   ✓ Prompt builder ready\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*80)\\n\",\n",
    "    \"print(\\\"RAG SYSTEM INITIALIZED\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*80)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Single Query Test\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def test_single_query(query: str, top_k: int = 3, show_context: bool = True):\\n\",\n",
    "    \"    \\\"\\\"\\\"Test a single query through the RAG pipeline\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n{'='*80}\\\")\\n\",\n",
    "    \"    print(f\\\"QUERY: {query}\\\")\\n\",\n",
    "    \"    print(f\\\"{'='*80}\\\\n\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Step 1: Embed query\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    query_embedding = embedder.embed_query(query)\\n\",\n",
    "    \"    embed_time = time.time() - start_time\\n\",\n",
    "    \"    print(f\\\"1. Query Embedding: {embed_time:.3f}s\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Step 2: Retrieve\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    retrieved_chunks = retriever.retrieve(query_embedding, k=top_k)\\n\",\n",
    "    \"    retrieval_time = time.time() - start_time\\n\",\n",
    "    \"    print(f\\\"2. Retrieval: {retrieval_time:.3f}s\\\")\\n\",\n",
    "    \"    print(f\\\"   Retrieved {len(retrieved_chunks)} chunks\\\\n\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show retrieved chunks\\n\",\n",
    "    \"    print(\\\"Retrieved Sources:\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 80)\\n\",\n",
    "    \"    for i, chunk in enumerate(retrieved_chunks, 1):\\n\",\n",
    "    \"        print(f\\\"\\\\n[{i}] Disease: {chunk['disease_name']}\\\")\\n\",\n",
    "    \"        print(f\\\"    Field: {chunk['field']}\\\")\\n\",\n",
    "    \"        print(f\\\"    Score: {chunk['score']:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"    URL: {chunk['url']}\\\")\\n\",\n",
    "    \"        print(f\\\"    Text: {chunk['text'][:200]}...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Step 3: Build context\\n\",\n",
    "    \"    context = retriever.format_context(retrieved_chunks)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if show_context:\\n\",\n",
    "    \"        print(f\\\"\\\\n{'='*80}\\\")\\n\",\n",
    "    \"        print(\\\"CONTEXT FOR LLM:\\\")\\n\",\n",
    "    \"        print(\\\"=\\\"*80)\\n\",\n",
    "    \"        print(context[:1000] + \\\"...\\\" if len(context) > 1000 else context)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Step 4: Build prompt\\n\",\n",
    "    \"    prompt = prompt_builder.build_prompt(query, context)\\n\",\n",
    "    \"    print(f\\\"\\\\n3. Prompt Length: {len(prompt)} characters\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Step 5: Generate response (if LLM available)\\n\",\n",
    "    \"    if llm_available:\\n\",\n",
    "    \"        print(\\\"\\\\n4. Generating response...\\\")\\n\",\n",
    "    \"        start_time = time.time()\\n\",\n",
    "    \"        answer = llm_generator.generate(prompt)\\n\",\n",
    "    \"        generation_time = time.time() - start_time\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"   Generation time: {generation_time:.2f}s\\\")\\n\",\n",
    "    \"        print(f\\\"\\\\n{'='*80}\\\")\\n\",\n",
    "    \"        print(\\\"GENERATED ANSWER:\\\")\\n\",\n",
    "    \"        print(\\\"=\\\"*80)\\n\",\n",
    "    \"        print(answer)\\n\",\n",
    "    \"        print(f\\\"\\\\n{'='*80}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        total_time = embed_time + retrieval_time + generation_time\\n\",\n",
    "    \"        print(f\\\"\\\\nTotal Time: {total_time:.2f}s\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return {\\n\",\n",
    "    \"            'query': query,\\n\",\n",
    "    \"            'retrieved_chunks': retrieved_chunks,\\n\",\n",
    "    \"            'answer': answer,\\n\",\n",
    "    \"            'embed_time': embed_time,\\n\",\n",
    "    \"            'retrieval_time': retrieval_time,\\n\",\n",
    "    \"            'generation_time': generation_time,\\n\",\n",
    "    \"            'total_time': total_time\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"\\\\n⚠ Skipping generation (LLM not available)\\\")\\n\",\n",
    "    \"        return {\\n\",\n",
    "    \"            'query': query,\\n\",\n",
    "    \"            'retrieved_chunks': retrieved_chunks,\\n\",\n",
    "    \"            'embed_time': embed_time,\\n\",\n",
    "    \"            'retrieval_time': retrieval_time\\n\",\n",
    "    \"        }\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test a single query\\n\",\n",
    "    \"test_query = \\\"What are the symptoms of malaria?\\\"\\n\",\n",
    "    \"result = test_single_query(test_query, top_k=3, show_context=False)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Multiple Queries Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define test queries\\n\",\n",
    "    \"test_queries = [\\n\",\n",
    "    \"    \\\"What are the symptoms of malaria?\\\",\\n\",\n",
    "    \"    \\\"How is tuberculosis treated?\\\",\\n\",\n",
    "    \"    \\\"What causes diabetes?\\\",\\n\",\n",
    "    \"    \\\"How can I prevent COVID-19?\\\",\\n\",\n",
    "    \"    \\\"What are the risk factors for heart disease?\\\",\\n\",\n",
    "    \"    \\\"How is HIV transmitted?\\\",\\n\",\n",
    "    \"    \\\"What are the complications of dengue fever?\\\",\\n\",\n",
    "    \"    \\\"How is cancer diagnosed?\\\",\\n\",\n",
    "    \"    \\\"What is the treatment for hypertension?\\\",\\n\",\n",
    "    \"    \\\"What are the symptoms of depression?\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Testing {len(test_queries)} queries...\\\\n\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Run all test queries\\n\",\n",
    "    \"results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, query in enumerate(test_queries, 1):\\n\",\n",
    "    \"    print(f\\\"\\\\n[{i}/{len(test_queries)}] Testing: {query}\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 80)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        result = test_single_query(query, top_k=3, show_context=False)\\n\",\n",
    "    \"        results.append(result)\\n\",\n",
    "    \"        print(\\\"✓ Success\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"✗ Error: {e}\\\")\\n\",\n",
    "    \"        results.append({'query': query, 'error': str(e)})\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    time.sleep(0.5)  # Brief pause between queries\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n\\\\nCompleted {len(results)} queries\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Performance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create performance DataFrame\\n\",\n",
    "    \"perf_data = []\\n\",\n",
    "    \"for result in results:\\n\",\n",
    "    \"    if 'error' not in result:\\n\",\n",
    "    \"        perf_data.append({\\n\",\n",
    "    \"            'query': result['query'][:50] + '...',\\n\",\n",
    "    \"            'embed_time': result['embed_time'],\\n\",\n",
    "    \"            'retrieval_time': result['retrieval_time'],\\n\",\n",
    "    \"            'generation_time': result.get('generation_time', 0),\\n\",\n",
    "    \"            'total_time': result.get('total_time', result['embed_time'] + result['retrieval_time'])\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"\\n\",\n",
    "    \"perf_df = pd.DataFrame(perf_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(perf_df) > 0:\\n\",\n",
    "    \"    print(\\\"Performance Statistics:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 80)\\n\",\n",
    "    \"    print(perf_df[['embed_time', 'retrieval_time', 'generation_time', 'total_time']].describe())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize timing breakdown\\n\",\n",
    "    \"if len(perf_df) > 0:\\n\",\n",
    "    \"    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Stacked bar chart\\n\",\n",
    "    \"    ax = axes[0]\\n\",\n",
    "    \"    perf_df[['embed_time', 'retrieval_time', 'generation_time']].plot(\\n\",\n",
    "    \"        kind='bar',\\n\",\n",
    "    \"        stacked=True,\\n\",\n",
    "    \"        ax=ax,\\n\",\n",
    "    \"        color=['#3498db', '#2ecc71', '#e74c3c']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    ax.set_xlabel('Query Index')\\n\",\n",
    "    \"    ax.set_ylabel('Time (seconds)')\\n\",\n",
    "    \"    ax.set_title('Time Breakdown by Query')\\n\",\n",
    "    \"    ax.legend(['Embedding', 'Retrieval', 'Generation'])\\n\",\n",
    "    \"    ax.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Box plot\\n\",\n",
    "    \"    ax = axes[1]\\n\",\n",
    "    \"    perf_df[['embed_time', 'retrieval_time', 'generation_time']].boxplot(ax=ax)\\n\",\n",
    "    \"    ax.set_ylabel('Time (seconds)')\\n\",\n",
    "    \"    ax.set_title('Time Distribution by Component')\\n\",\n",
    "    \"    ax.set_xticklabels(['Embedding', 'Retrieval', 'Generation'])\\n\",\n",
    "    \"    ax.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Retrieval Quality Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze retrieval scores\\n\",\n",
    "    \"all_scores = []\\n\",\n",
    "    \"for result in results:\\n\",\n",
    "    \"    if 'retrieved_chunks' in result:\\n\",\n",
    "    \"        for chunk in result['retrieved_chunks']:\\n\",\n",
    "    \"            all_scores.append(chunk['score'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"if all_scores:\\n\",\n",
    "    \"    scores_array = np.array(all_scores)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Retrieval Score Statistics:\\\")\\n\",\n",
    "    \"    print(f\\\"Mean: {scores_array.mean():.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Std:  {scores_array.std():.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Min:  {scores_array.min():.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Max:  {scores_array.max():.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Distribution\\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    plt.hist(all_scores, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\\n\",\n",
    "    \"    plt.axvline(scores_array.mean(), color='red', linestyle='--', label=f'Mean: {scores_array.mean():.3f}')\\n\",\n",
    "    \"    plt.xlabel('Similarity Score')\\n\",\n",
    "    \"    plt.ylabel('Frequency')\\n\",\n",
    "    \"    plt.title('Distribution of Retrieval Similarity Scores')\\n\",\n",
    "    \"    plt.legend()\\n\",\n",
    "    \"    plt.grid(alpha=0.3)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze retrieved diseases\\n\",\n",
    "    \"disease_counts = {}\\n\",\n",
    "    \"for result in results:\\n\",\n",
    "    \"    if 'retrieved_chunks' in result:\\n\",\n",
    "    \"        for chunk in result['retrieved_chunks']:\\n\",\n",
    "    \"            disease = chunk['disease_name']\\n\",\n",
    "    \"            disease_counts[disease] = disease_counts.get(disease, 0) + 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"if disease_counts:\\n\",\n",
    "    \"    # Top retrieved diseases\\n\",\n",
    "    \"    top_diseases = sorted(disease_counts.items(), key=lambda x: x[1], reverse=True)[:10]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nTop 10 Most Retrieved Diseases:\\\")\\n\",\n",
    "    \"    for disease, count in top_diseases:\\n\",\n",
    "    \"        print(f\\\"  {disease}: {count} times\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize\\n\",\n",
    "    \"    diseases, counts = zip(*top_diseases)\\n\",\n",
    "    \"    plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(diseases)))\\n\",\n",
    "    \"    plt.barh(diseases, counts, color=colors)\\n\",\n",
    "    \"    plt.xlabel('Retrieval Count')\\n\",\n",
    "    \"    plt.ylabel('Disease')\\n\",\n",
    "    \"    plt.title('Most Frequently Retrieved Diseases')\\n\",\n",
    "    \"    plt.grid(axis='x', alpha=0.3)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Response Quality Inspection\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display sample responses\\n\",\n",
    "    \"if llm_available:\\n\",\n",
    "    \"    print(\\\"Sample Generated Responses:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 80)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, result in enumerate(results[:3], 1):\\n\",\n",
    "    \"        if 'answer' in result:\\n\",\n",
    "    \"            print(f\\\"\\\\n{i}. Query: {result['query']}\\\")\\n\",\n",
    "    \"            print(\\\"-\\\" * 80)\\n\",\n",
    "    \"            print(f\\\"Answer: {result['answer']}\\\")\\n\",\n",
    "    \"            print(\\\"=\\\" * 80)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Response generation skipped (LLM not available)\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Custom Query Testing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Interactive query testing\\n\",\n",
    "    \"print(\\\"Enter your own query to test the RAG system:\\\")\\n\",\n",
    "    \"print(\\\"(or press Enter to skip)\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"custom_query = input(\\\"Your query: \\\").strip()\\n\",\n",
    "    \"\\n\",\n",
    "    \"if custom_query:\\n\",\n",
    "    \"    result = test_single_query(custom_query, top_k=5, show_context=True)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Skipped custom query\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. System Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*80)\\n\",\n",
    "    \"print(\\\"RAG SYSTEM TEST SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*80)\\n\",\n",
    "    \"\\n\",\n",
    "    \"success_count = len([r for r in results if 'error' not in r])\\n\",\n",
    "    \"error_count = len([r for r in results if 'error' in r])\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nTest Queries: {len(test_queries)}\\\")\\n\",\n",
    "    \"print(f\\\"Successful: {success_count}\\\")\\n\",\n",
    "    \"print(f\\\"Failed: {error_count}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(perf_df) > 0:\\n\",\n",
    "    \"    print(f\\\"\\\\nPerformance:\\\")\\n\",\n",
    "    \"    print(f\\\"  Average embedding time: {perf_df['embed_time'].mean():.3f}s\\\")\\n\",\n",
    "    \"    print(f\\\"  Average retrieval time: {perf_df['retrieval_time'].mean():.3f}s\\\")\\n\",\n",
    "    \"    if 'generation_time' in perf_df.columns:\\n\",\n",
    "    \"        print(f\\\"  Average generation time: {perf_df['generation_time'].mean():.3f}s\\\")\\n\",\n",
    "    \"        print(f\\\"  Average total time: {perf_df['total_time'].mean():.3f}s\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if all_scores:\\n\",\n",
    "    \"    print(f\\\"\\\\nRetrieval Quality:\\\")\\n\",\n",
    "    \"    print(f\\\"  Average similarity score: {np.mean(all_scores):.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"  Min similarity score: {np.min(all_scores):.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"  Max similarity score: {np.max(all_scores):.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nLLM Status: {'Available' if llm_available else 'Not Available'}\\\")\\n\",\n",
    "    \"print(f\\\"Device: {settings.DEVICE}\\\")\\n\",\n",
    "    \"print(f\\\"Top-K Retrieval: {settings.TOP_K_RETRIEVAL}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*80)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Export Results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save test results\\n\",\n",
    "    \"output_path = Path('../data/processed/rag_test_results.json')\\n\",\n",
    "    \"output_path.parent.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare results for JSON (remove non-serializable objects)\\n\",\n",
    "    \"export_results = []\\n\",\n",
    "    \"for result in results:\\n\",\n",
    "    \"    export_result = {\\n\",\n",
    "    \"        'query': result.get('query', ''),\\n\",\n",
    "    \"        'answer': result.get('answer', ''),\\n\",\n",
    "    \"        'embed_time': result.get('embed_time', 0),\\n\",\n",
    "    \"        'retrieval_time': result.get('retrieval_time', 0),\\n\",\n",
    "    \"        'generation_time': result.get('generation_time', 0),\\n\",\n",
    "    \"        'error': result.get('error', None)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    if 'retrieved_chunks' in result:\\n\",\n",
    "    \"        export_result['retrieved_diseases'] = [\\n\",\n",
    "    \"            {\\n\",\n",
    "    \"                'disease': chunk['disease_name'],\\n\",\n",
    "    \"                'score': chunk['score']\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"            for chunk in result['retrieved_chunks']\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"    export_results.append(export_result)\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(output_path, 'w') as f:\\n\",\n",
    "    \"    json.dump(export_results, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"✓ Results saved to {output_path}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
