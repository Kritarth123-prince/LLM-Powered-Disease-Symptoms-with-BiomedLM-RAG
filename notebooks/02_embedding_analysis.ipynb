{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Embedding Analysis\\n\",\n",
    "    \"## Analyze embeddings generated from WHO dataset\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook analyzes:\\n\",\n",
    "    \"- Embedding generation process\\n\",\n",
    "    \"- Vector space visualization\\n\",\n",
    "    \"- Clustering analysis\\n\",\n",
    "    \"- Similarity patterns\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.cluster import KMeans\\n\",\n",
    "    \"from sklearn.metrics.pairwise import cosine_similarity\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.embedding.model_loader import ModelLoader\\n\",\n",
    "    \"from src.embedding.embedder import EmbeddingGenerator\\n\",\n",
    "    \"from src.config import settings\\n\",\n",
    "    \"\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (12, 8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"✓ Imports successful\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Chunks and Embeddings\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load chunks\\n\",\n",
    "    \"chunks_path = Path('../data/processed/chunks.json')\\n\",\n",
    "    \"if chunks_path.exists():\\n\",\n",
    "    \"    with open(chunks_path, 'r') as f:\\n\",\n",
    "    \"        chunks = json.load(f)\\n\",\n",
    "    \"    print(f\\\"✓ Loaded {len(chunks)} chunks\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"❌ Chunks not found! Run: python scripts/build_index.py\\\")\\n\",\n",
    "    \"    chunks = []\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load embeddings\\n\",\n",
    "    \"embeddings_path = Path('../data/processed/embeddings.npy')\\n\",\n",
    "    \"if embeddings_path.exists():\\n\",\n",
    "    \"    embeddings = np.load(embeddings_path)\\n\",\n",
    "    \"    print(f\\\"✓ Loaded embeddings with shape: {embeddings.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"  - Dimension: {embeddings.shape[1]}\\\")\\n\",\n",
    "    \"    print(f\\\"  - Number of vectors: {embeddings.shape[0]}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"❌ Embeddings not found! Run: python scripts/build_index.py\\\")\\n\",\n",
    "    \"    embeddings = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Embedding Statistics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if embeddings is not None:\\n\",\n",
    "    \"    print(\\\"Embedding Statistics:\\\")\\n\",\n",
    "    \"    print(f\\\"Mean: {embeddings.mean():.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"Std:  {embeddings.std():.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"Min:  {embeddings.min():.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"Max:  {embeddings.max():.6f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Check if normalized\\n\",\n",
    "    \"    norms = np.linalg.norm(embeddings, axis=1)\\n\",\n",
    "    \"    print(f\\\"\\\\nVector norms (should be ~1 if normalized):\\\")\\n\",\n",
    "    \"    print(f\\\"Mean norm: {norms.mean():.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"Std norm:  {norms.std():.6f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Distribution of embedding values\\n\",\n",
    "    \"if embeddings is not None:\\n\",\n",
    "    \"    plt.figure(figsize=(14, 5))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sample some dimensions for visualization\\n\",\n",
    "    \"    sample_dims = np.random.choice(embeddings.shape[1], 5, replace=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, dim in enumerate(sample_dims):\\n\",\n",
    "    \"        plt.subplot(1, 5, i+1)\\n\",\n",
    "    \"        plt.hist(embeddings[:, dim], bins=50, alpha=0.7, color=f'C{i}')\\n\",\n",
    "    \"        plt.title(f'Dim {dim}')\\n\",\n",
    "    \"        plt.xlabel('Value')\\n\",\n",
    "    \"        if i == 0:\\n\",\n",
    "    \"            plt.ylabel('Frequency')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.suptitle('Distribution of Sample Embedding Dimensions', fontsize=14, fontweight='bold', y=1.02)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Dimensionality Reduction - PCA\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if embeddings is not None:\\n\",\n",
    "    \"    print(\\\"Running PCA...\\\")\\n\",\n",
    "    \"    pca = PCA(n_components=50)\\n\",\n",
    "    \"    embeddings_pca = pca.fit_transform(embeddings)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Explained variance\\n\",\n",
    "    \"    cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Individual variance\\n\",\n",
    "    \"    plt.subplot(1, 2, 1)\\n\",\n",
    "    \"    plt.bar(range(1, 51), pca.explained_variance_ratio_[:50], alpha=0.7, color='skyblue')\\n\",\n",
    "    \"    plt.xlabel('Principal Component')\\n\",\n",
    "    \"    plt.ylabel('Explained Variance Ratio')\\n\",\n",
    "    \"    plt.title('PCA - Individual Variance Explained')\\n\",\n",
    "    \"    plt.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Cumulative variance\\n\",\n",
    "    \"    plt.subplot(1, 2, 2)\\n\",\n",
    "    \"    plt.plot(range(1, 51), cumsum_variance[:50], marker='o', linewidth=2, markersize=4)\\n\",\n",
    "    \"    plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\\n\",\n",
    "    \"    plt.xlabel('Number of Components')\\n\",\n",
    "    \"    plt.ylabel('Cumulative Explained Variance')\\n\",\n",
    "    \"    plt.title('PCA - Cumulative Variance Explained')\\n\",\n",
    "    \"    plt.legend()\\n\",\n",
    "    \"    plt.grid(alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find components needed for 95% variance\\n\",\n",
    "    \"    n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\\n\",\n",
    "    \"    print(f\\\"\\\\nComponents needed for 95% variance: {n_components_95}\\\")\\n\",\n",
    "    \"    print(f\\\"Variance explained by first 2 components: {cumsum_variance[1]:.2%}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. 2D Visualization with PCA\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if embeddings is not None and chunks:\\n\",\n",
    "    \"    # PCA to 2D\\n\",\n",
    "    \"    pca_2d = PCA(n_components=2)\\n\",\n",
    "    \"    embeddings_2d = pca_2d.fit_transform(embeddings)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract disease names for coloring\\n\",\n",
    "    \"    disease_names = [chunk['disease_name'] for chunk in chunks]\\n\",\n",
    "    \"    unique_diseases = list(set(disease_names))\\n\",\n",
    "    \"    disease_to_idx = {d: i for i, d in enumerate(unique_diseases)}\\n\",\n",
    "    \"    colors = [disease_to_idx[d] for d in disease_names]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(14, 10))\\n\",\n",
    "    \"    scatter = plt.scatter(\\n\",\n",
    "    \"        embeddings_2d[:, 0], \\n\",\n",
    "    \"        embeddings_2d[:, 1],\\n\",\n",
    "    \"        c=colors,\\n\",\n",
    "    \"        cmap='tab20',\\n\",\n",
    "    \"        alpha=0.6,\\n\",\n",
    "    \"        s=30\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\\n\",\n",
    "    \"    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\\n\",\n",
    "    \"    plt.title('2D PCA Visualization of Embeddings', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    plt.grid(alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show a few disease names\\n\",\n",
    "    \"    if len(unique_diseases) <= 20:\\n\",\n",
    "    \"        for disease in unique_diseases[:10]:\\n\",\n",
    "    \"            idx = disease_names.index(disease)\\n\",\n",
    "    \"            plt.annotate(\\n\",\n",
    "    \"                disease,\\n\",\n",
    "    \"                (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\\n\",\n",
    "    \"                fontsize=8,\\n\",\n",
    "    \"                alpha=0.7\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. t-SNE Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if embeddings is not None and len(embeddings) < 5000:\\n\",\n",
    "    \"    print(\\\"Running t-SNE (this may take a few minutes)...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Use PCA first for speed\\n\",\n",
    "    \"    pca_50 = PCA(n_components=50)\\n\",\n",
    "    \"    embeddings_pca_50 = pca_50.fit_transform(embeddings)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\\n\",\n",
    "    \"    embeddings_tsne = tsne.fit_transform(embeddings_pca_50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(14, 10))\\n\",\n",
    "    \"    scatter = plt.scatter(\\n\",\n",
    "    \"        embeddings_tsne[:, 0],\\n\",\n",
    "    \"        embeddings_tsne[:, 1],\\n\",\n",
    "    \"        c=colors,\\n\",\n",
    "    \"        cmap='tab20',\\n\",\n",
    "    \"        alpha=0.6,\\n\",\n",
    "    \"        s=30\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.xlabel('t-SNE Dimension 1', fontsize=12)\\n\",\n",
    "    \"    plt.ylabel('t-SNE Dimension 2', fontsize=12)\\n\",\n",
    "    \"    plt.title('t-SNE Visualization of Embeddings', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    plt.grid(alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"✓ t-SNE visualization complete\\\")\\n\",\n",
    "    \"elif embeddings is not None:\\n\",\n",
    "    \"    print(f\\\"Skipping t-SNE: too many samples ({len(embeddings)}). Use a subset for t-SNE.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Clustering Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if embeddings is not None:\\n\",\n",
    "    \"    # Determine optimal number of clusters using elbow method\\n\",\n",
    "    \"    max_clusters = min(20, len(embeddings) // 10)\\n\",\n",
    "    \"    inertias = []\\n\",\n",
    "    \"    K_range = range(2, max_clusters + 1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Testing K-means with K from 2 to {max_clusters}...\\\")\\n\",\n",
    "    \"    for k in K_range:\\n\",\n",
    "    \"        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\\n\",\n",
    "    \"        kmeans.fit(embeddings)\\n\",\n",
    "    \"        inertias.append(kmeans.inertia_)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    plt.plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\\n\",\n",
    "    \"    plt.xlabel('Number of Clusters (K)', fontsize=12)\\n\",\n",
    "    \"    plt.ylabel('Inertia', fontsize=12)\\n\",\n",
    "    \"    plt.title('K-Means Elbow Method', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    plt.grid(alpha=0.3)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Apply K-means with chosen K\\n\",\n",
    "    \"if embeddings is not None:\\n\",\n",
    "    \"    n_clusters = 8\\n\",\n",
    "    \"    print(f\\\"Applying K-means with {n_clusters} clusters...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\\n\",\n",
    "    \"    cluster_labels = kmeans.fit_predict(embeddings)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add cluster labels to chunks\\n\",\n",
    "    \"    for i, chunk in enumerate(chunks):\\n\",\n",
    "    \"        chunk['cluster'] = int(cluster_labels[i])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Analyze clusters\\n\",\n",
    "    \"    cluster_df = pd.DataFrame(chunks)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nCluster Distribution:\\\")\\n\",\n",
    "    \"    cluster_counts = cluster_df['cluster'].value_counts().sort_index()\\n\",\n",
    "    \"    print(cluster_counts)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize cluster sizes\\n\",\n",
    "    \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"    colors_bar = plt.cm.Set3(range(n_clusters))\\n\",\n",
    "    \"    plt.bar(cluster_counts.index, cluster_counts.values, color=colors_bar, edgecolor='black')\\n\",\n",
    "    \"    plt.xlabel('Cluster ID', fontsize=12)\\n\",\n",
    "    \"    plt.ylabel('Number of Chunks', fontsize=12)\\n\",\n",
    "    \"    plt.title('Cluster Size Distribution', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    plt.xticks(cluster_counts.index)\\n\",\n",
    "    \"    plt.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Show sample diseases from each cluster\\n\",\n",
    "    \"if embeddings is not None:\\n\",\n",
    "    \"    print(\\\"\\\\nSample Diseases from Each Cluster:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*80)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for cluster_id in range(n_clusters):\\n\",\n",
    "    \"        cluster_chunks = cluster_df[cluster_df['cluster'] == cluster_id]\\n\",\n",
    "    \"        unique_diseases = cluster_chunks['disease_name'].unique()[:5]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\nCluster {cluster_id} ({len(cluster_chunks)} chunks):\\\")\\n\",\n",
    "    \"        for disease in unique_diseases:\\n\",\n",
    "    \"            print(f\\\"  - {disease}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Similarity Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if embeddings is not None and len(embeddings) < 1000:\\n\",\n",
    "    \"    print(\\\"Computing cosine similarity matrix...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Compute similarity for subset\\n\",\n",
    "    \"    sample_size = min(100, len(embeddings))\\n\",\n",
    "    \"    sample_indices = np.random.choice(len(embeddings), sample_size, replace=False)\\n\",\n",
    "    \"    sample_embeddings = embeddings[sample_indices]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    similarity_matrix = cosine_similarity(sample_embeddings)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(12, 10))\\n\",\n",
    "    \"    sns.heatmap(\\n\",\n",
    "    \"        similarity_matrix,\\n\",\n",
    "    \"        cmap='coolwarm',\\n\",\n",
    "    \"        center=0,\\n\",\n",
    "    \"        square=True,\\n\",\n",
    "    \"        linewidths=0,\\n\",\n",
    "    \"        cbar_kws={\\\"shrink\\\": 0.8}\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    plt.title(f'Cosine Similarity Matrix (Sample of {sample_size} chunks)', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    plt.xlabel('Chunk Index')\\n\",\n",
    "    \"    plt.ylabel('Chunk Index')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Statistics\\n\",\n",
    "    \"    # Exclude diagonal\\n\",\n",
    "    \"    mask = np.ones_like(similarity_matrix, dtype=bool)\\n\",\n",
    "    \"    np.fill_diagonal(mask, False)\\n\",\n",
    "    \"    off_diagonal_sims = similarity_matrix[mask]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nSimilarity Statistics (excluding self-similarity):\\\")\\n\",\n",
    "    \"    print(f\\\"Mean similarity: {off_diagonal_sims.mean():.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Std similarity:  {off_diagonal_sims.std():.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Min similarity:  {off_diagonal_sims.min():.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Max similarity:  {off_diagonal_sims.max():.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Query Similarity Test\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load embedding model for query testing\\n\",\n",
    "    \"print(\\\"Loading embedding model for query testing...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"model_loader = ModelLoader(settings.EMBEDDING_MODEL, settings.DEVICE)\\n\",\n",
    "    \"tokenizer, model = model_loader.load()\\n\",\n",
    "    \"embedder = EmbeddingGenerator(tokenizer, model, settings.DEVICE)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"✓ Model loaded\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test query\\n\",\n",
    "    \"test_query = \\\"What are the symptoms of malaria?\\\"\\n\",\n",
    "    \"print(f\\\"Test Query: '{test_query}'\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate query embedding\\n\",\n",
    "    \"query_embedding = embedder.embed_query(test_query)\\n\",\n",
    "    \"print(f\\\"Query embedding shape: {query_embedding.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compute similarities\\n\",\n",
    "    \"similarities = cosine_similarity([query_embedding], embeddings)[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get top 10 matches\\n\",\n",
    "    \"top_indices = np.argsort(similarities)[::-1][:10]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nTop 10 Most Similar Chunks:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*80)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for rank, idx in enumerate(top_indices, 1):\\n\",\n",
    "    \"    chunk = chunks[idx]\\n\",\n",
    "    \"    sim_score = similarities[idx]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n{rank}. Disease: {chunk['disease_name']}\\\")\\n\",\n",
    "    \"    print(f\\\"   Field: {chunk['field']}\\\")\\n\",\n",
    "    \"    print(f\\\"   Similarity: {sim_score:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Text: {chunk['text'][:150]}...\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if embeddings is not None:\\n\",\n",
    "    \"    print(\\\"\\\\n\\\" + \\\"=\\\"*80)\\n\",\n",
    "    \"    print(\\\"EMBEDDING ANALYSIS SUMMARY\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*80)\\n\",\n",
    "    \"    print(f\\\"Total Embeddings: {len(embeddings)}\\\")\\n\",\n",
    "    \"    print(f\\\"Embedding Dimension: {embeddings.shape[1]}\\\")\\n\",\n",
    "    \"    print(f\\\"\\\\nPCA Analysis:\\\")\\n\",\n",
    "    \"    print(f\\\"  - Components for 95% variance: {n_components_95}\\\")\\n\",\n",
    "    \"    print(f\\\"  - First 2 components variance: {cumsum_variance[1]:.2%}\\\")\\n\",\n",
    "    \"    print(f\\\"\\\\nClustering:\\\")\\n\",\n",
    "    \"    print(f\\\"  - Number of clusters: {n_clusters}\\\")\\n\",\n",
    "    \"    print(f\\\"  - Largest cluster size: {cluster_counts.max()}\\\")\\n\",\n",
    "    \"    print(f\\\"  - Smallest cluster size: {cluster_counts.min()}\\\")\\n\",\n",
    "    \"    print(\\\"\\\\n\\\" + \\\"=\\\"*80)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
